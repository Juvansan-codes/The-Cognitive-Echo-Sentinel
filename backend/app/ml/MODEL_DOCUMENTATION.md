# ğŸ§  Acoustic Risk Model â€” Training & Integration Documentation

> **Project:** Cognitive Echo Sentinel  
> **Model Purpose:** Predict cognitive risk from speech/voice features  
> **Date:** February 27, 2026

---

## ğŸ“Š 1. Dataset

**Name:** Parkinson Multiple Sound Recording  
**Location:** `backend/Parkinson_Multiple_Sound_Recording/`

| File | Rows | Columns | Used |
|---|---|---|---|
| `train_data.txt` | 1,040 | 29 (28 features + 1 label) | âœ… Yes |
| `test_data.txt` | 168 | 28 (mismatched) | âŒ Dropped |

- **No header row** â€” all values are numeric, comma-separated
- **Label column** (last column): `1` = Parkinson's, `0` = Healthy
- **Class balance:** 520 healthy + 520 Parkinson's (perfectly balanced)
- `test_data.txt` was excluded because it had 28 columns (vs 29) and contained only class `1`

### Feature Breakdown (28 columns)

| Index | Feature | Description |
|---|---|---|
| f0 | Subject ID | Patient identifier (1â€“40) |
| f1â€“f5 | Jitter variants | Jitter(%), Jitter(Abs), RAP, PPQ5, DDP |
| f6â€“f11 | Shimmer variants | Shimmer, Shimmer(dB), APQ3, APQ5, APQ11, DDA |
| f12 | Autocorrelation | Voice regularity |
| f13 | Noise-to-Harmonics | Inverse of HNR |
| f14 | Harmonics-to-Noise | Voice clarity (HNR) |
| f15â€“f19 | Pitch statistics | Median, Mean, StdDev, Min, Max pitch |
| f20â€“f21 | Pulse counts | Number of pulses, Number of periods |
| f22â€“f23 | Period stats | Mean period, StdDev period |
| f24 | Fraction unvoiced | % of unvoiced frames |
| f25â€“f26 | Voice breaks | Count and degree of breaks |
| f27 | UPDRS score | Clinical motor score |

---

## ğŸ”¬ 2. Training Pipeline

**Script:** `backend/app/ml/train_acoustic_model.py`

### How to Run

```bash
cd backend
python -m app.ml.train_acoustic_model \
    --dataset "./Parkinson_Multiple_Sound_Recording" \
    --output  "./app/models"
```

### Pipeline Steps

1. **Load Data** â€” Reads `train_data.txt` with pandas, auto-detects headerless CSV, assigns column names
2. **Handle Mismatches** â€” If train/test have different column counts, keeps only matching files
3. **Clean Data** â€” Coerces to numeric, fills NaN with column medians, drops remaining bad rows
4. **Encode Labels** â€” `LabelEncoder` maps class names to integers
5. **Split** â€” Stratified 80/20 train/test split (`random_state=42`)
6. **Scale** â€” `StandardScaler` normalizes all features
7. **Train** â€” `RandomForestClassifier` with these hyperparameters:

| Parameter | Value | Why |
|---|---|---|
| `n_estimators` | 200 | Enough trees for stable predictions |
| `max_depth` | 10 | Prevents overfitting on small dataset |
| `class_weight` | `balanced` | Handles any class imbalance |
| `n_jobs` | -1 | Uses all CPU cores |
| `random_state` | 42 | Reproducibility |

8. **Evaluate** â€” Accuracy, precision, recall, F1, confusion matrix, top-10 feature importances
9. **Save Artifacts** â€” 4 `.pkl` files via `joblib`

### Dependencies

```
pandas
scikit-learn
joblib
numpy
```

---

## ğŸ“¦ 3. Saved Model Artifacts

All saved to `backend/app/models/`:

| File | Size | Purpose |
|---|---|---|
| `neuro_risk_model.pkl` | 501 KB | Full pipeline (StandardScaler + RandomForest) |
| `scaler.pkl` | 1.2 KB | Standalone scaler for feature normalization |
| `label_encoder.pkl` | 0.5 KB | Maps encoded predictions back to class names |
| `feature_names.pkl` | 0.4 KB | List of 28 feature names for validation |

---

## ğŸ”— 4. Runtime Integration

**Modified file:** `backend/app/services/risk_engine.py`

### Model Loading (Global Scope)

```python
# Loaded ONCE at import time, not per-request
_ml_model = joblib.load("neuro_risk_model.pkl")
_ml_scaler = joblib.load("scaler.pkl")
_ml_label_encoder = joblib.load("label_encoder.pkl")
```

If loading fails â†’ `_ml_ready = False` â†’ automatic heuristic fallback.

### Feature Mapping (Live Audio â†’ Model Input)

The live audio pipeline extracts **10 named features** (jitter, shimmer, pitch, HNR, etc.).  
The model expects **28 numeric features** from the Parkinson dataset.

`build_model_feature_vector()` maps between them:

| Live Feature | â†’ Model Features |
|---|---|
| `jitter_percent` | f1â€“f5 (jitter variants derived mathematically) |
| `shimmer_percent` | f6â€“f11 (shimmer variants, scaled 3Ã—) |
| `pitch_stability` | f12 (autocorrelation) |
| `harmonics_to_noise` | f13 (inverted), f14 (direct) |
| `mean_pitch_hz`, `pitch_std_hz` | f15â€“f19 (pitch statistics + derived min/max) |
| `speech_rate` | f20â€“f21 (pulse/period counts) |
| `pause_ratio` | f24â€“f26 (fraction unvoiced, voice breaks) |
| N/A | f0 = 20.5, f27 = 13.0 (global training means) |

> **Why global means for f0 and f27?** These features (subject ID, UPDRS motor score) are dataset artifacts â€” they can't be extracted from live audio. Setting them to the global mean makes them neutral so the model judges based on actual acoustic features.

### Scoring Strategy: Blended (40% ML + 60% Heuristic)

```
Final Risk = (ML Probability Score Ã— 0.40) + (Heuristic Score Ã— 0.60)
```

**Why blend instead of pure ML?**

The model's top discriminators from training were:
- `feature_0` (Subject ID) â€” 48.4% importance
- `feature_27` (UPDRS) â€” 38.2% importance

These are **non-acoustic** and can't be extracted from live audio. With them neutralized, the model's acoustic-only discrimination is limited (~60% vs 63% probability for healthy vs risky inputs).

The heuristic uses sigmoid-weighted jitter, shimmer, stability, HNR, and baseline deviation â€” providing reliable feature-level differentiation.

**Blending gives us the best of both:**
- ML contributes its learned patterns from 1,040 real patient recordings
- Heuristic ensures proper risk separation based on individual feature values

### Validation Results

| Input Profile | ML Score | Heuristic Score | **Blended** |
|---|---|---|---|
| Healthy (low jitter, high stability) | 64.2 | 5.2 | **28.9** âœ… Low |
| Moderate (mid-range features) | 66.7 | 36.4 | **48.5** âš ï¸ Medium |
| At-Risk (high jitter, low HNR) | 68.6 | 79.2 | **74.9** ğŸ”´ High |

### Error Handling

| Failure | Behavior |
|---|---|
| Model files missing | Heuristic-only scoring |
| joblib import fails | Heuristic-only scoring |
| Feature length mismatch | Logged warning â†’ heuristic fallback |
| NaN/Inf in features | Replaced with 0.0 before inference |
| predict_proba fails | Caught â†’ heuristic fallback |
| **API never crashes** | âœ… All failures are gracefully handled |

---

## ğŸ—ï¸ 5. Architecture Flow

```
Audio File Upload
       â”‚
       â–¼
 extract_features()        â† librosa / parselmouth
       â”‚
       â”œâ”€â”€â†’ compare_to_baseline()
       â”‚
       â–¼
 compute_acoustic_risk()   â† ML model + heuristic blend
       â”‚
       â”œâ”€â”€â†’ run_lexical_analysis()  â† Featherless AI (async)
       â”‚
       â–¼
 compute_final_neuro_risk()
       â”‚
       â–¼
 JSON Response: { acoustic_risk, cognitive_risk, neuro_risk_level }
```

---

## ğŸ”® 6. Future Improvements

1. **Retrain on audio-extracted features** â€” Train on features extracted by our own pipeline (librosa/parselmouth) rather than precomputed tabular data, eliminating the feature mapping layer
2. **Collect real user data** â€” Build a dataset from actual app recordings for domain-specific accuracy
3. **Remove dataset artifacts** â€” Drop subject ID and UPDRS from training to force the model to learn from acoustic features only
4. **Add cross-validation** â€” Use k-fold CV during training for more robust evaluation
5. **Upgrade to XGBoost/LightGBM** â€” Gradient boosting may outperform Random Forest on this dataset size
